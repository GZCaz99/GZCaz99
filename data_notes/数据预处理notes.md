# 机器学习项目的数据处理笔记
在建模前,需要对数据集进行处理,这也是所有学习项目(分类/回归)中最重要且一般耗时最多的部分. 一般来说,每个项目都要求我们根据若干个数据集中提取数据进行建模,常见工作流程如下:
   
   1. 观察数据集,看看是否有重复的特征列,以及各特征列的分布/类型等(EDA阶段)
        - df.head()/df.info()/df.describe()/df.display()..
        - 观察各特征列的类型
        - 观察缺失值情况,选择合适的填充方法或者直接去掉
            - scikitlearn 中的 simple imputer 包
               > https://cloud.tencent.com/developer/article/1786924
            - LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。
            - 直接删去(如果占比不多可以考虑,如果某一列的缺失值太多则考虑直接去掉)
            - 异常值(outlier), 观察最大/最小值, 3 $\sigma$ 原则, 箱型图、直方图（尾巴）、散点图（常用）, 方差（离目标远近程度，对噪声敏感）、分位图（0.5%~99.5%）四分位数间距：25%~75%之间的区间宽度。

        - 观察各个特征列中值的分布情况
            
            - 核密度估计: seaborn.violinplot/histplot...
            - 单个特征：连续性、分布情况，包括偏度（skewness）和峰度（kurtosis）/ 多个特征:可以简单回归一下
            - correlation, 热点图
            - 观察label的分布情况: 如果df1中每个x对应一个label, df2中又对应另一个label.../根据项目中不同数据集的特点想办法合并成为一个训练集
            - 对 Numerical Variable，可以用 Box Plot 来直观地查看它的分布。对于坐标类数据，可以用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在。对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。

        - 数据预处理
            - from sklearn.preprocessing import ...
            - 探索数据的统计信息，最值、四分位数、均值、方差
            - 数据取值范围缩放: 数据标准化（Standardization）用到的多,数据归一化（Scaling）用的少,数据正规化（Normalization）
            - Standardization: sklearn.preprocessing.StandardScaler,转换为Z-score，使数值特征列的算数平均为0，方差（以及标准差）为1。不免疫outlier。或者 如果数值特征列中存在数值极大或极小的outlier（通过EDA发现），应该使用更稳健（robust）的统计数据：用中位数而不是算术平均数，用分位数（quantile）而不是方差。这种标准化方法有一个重要的参数：（分位数下限，分位数上限），最好通过EDA的数据可视化确定。免疫outlier。(sklearn.preprocessing.RobustScaler)
            - scaling:将一列的数值，除以这一列的最大绝对值。不免疫outlier。
               -  sklearn.preprocessing.MaxAbsScaler
               -  preprocessing.MinMaxScaler, 将属性缩放到一个指定的最大值和最小值(通常是1-0)之间,对于方差非常小的属性可以增强其稳定性
            - normalization归一化: sklearn.preprocessing.Normalizer 奇异样本数据的存在会引起训练时间增大，同时也可能导致无法收敛，因此，当存在奇异样本数据时，在进行训练之前需要对预处理数据进行归一化；反之，不存在奇异样本数据时，则可以不进行归一化。
               - 简而言之，归一化的**目的**就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。归一化的**过程**是将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性这个方法会很有用. Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础.Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。https://zhuanlan.zhihu.com/p/424518359
               - 或者在特征数据取值范围相差太大时,也需要运用归一化. e.g. 房价(y), 房间数(0-10),面积（20-1000）
               - 加快了梯度下降求最优解的速度/有可能提高精度
               - 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、gbdt、xgboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。
         - 数据采样：采集、清洗过数据以后，正负样本可能是不均衡的，要进行数据采样。采样的方法有随机采样和分层抽样。但是随机采样会有隐患，因为可能某次随机采样得到的数据很不均匀，更多的是根据特征采用分层抽样。
            - 分层采样
            -  正负样本不平衡处理办法：
               >正样本 >> 负样本，且量都挺大 => downsampling（下采样，去除一些正例，使得正例和反例数量接近）
               >正样本 >> 负样本，量不大 => 采集更多的数据/上采样/oversampling(比如图像识别中的镜像和旋转) -增加反例使得正例和反例数量接近/修改损失函数/loss function (设置样本权重)

                
        
  
  2. 特征工程

     - 特征编码
          - polynomial: 它是使用多项式的方法来进行的，如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2）https://blog.csdn.net/xiaohutong1991/article/details/107945459
          - https://blog.csdn.net/weixin_38002569/article/details/83095512
     - 数值型特征:
          - 做对数log变换(train.SalePrice = np.loglp(train.SalePrice)) , 多项式扩展数值特征
            其实就是多项式编码
         - 离散化：把连续值转成非线性数据。例如电商会有各种连续的价格表，从0.03到100元，假如以一元钱的间距分割成99个区间，用99维的向量代表每一个价格所处的区间，1.2元和1.6元的向量都是 [0,1,0,…,0]。pd.cut() 可以直接把数据分成若干段
         - 观察柱状分布：离散化后统计每个区间的个数做柱状图。
            
     - 分类特征:
        - 二值化(binomial),用0/1代表不同类(e.g, iris 是个三分类数据集,但可以用1来表示第一和第二类,用2来表示第三类), 对于同时属于多类的label,可以参考
        - label encoder, 用有序数据对**不连续**的label 编号, e.g. iris中的花中类, vertosa 设为 3, sentosa 设为 2 ...
        - one-hot encoding, 一般是多类label/类之间没有顺序联系时用, 在数据集中创建一个新的df,有多少类这个数据就有多少维, e.g. 有6类数据, 200行, 维度就是6*200. 
        - meanencoding, 针对高基数类别特征的有监督编码,针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。优点：和独热编码相比，节省内存、减少算法计算时间、有效增强模型表现。
        - 在类别特征列里，有时会有一些类别，在训练集和测试集中总共只出现一次，例如特别偏僻的郊区地址。此时保留其原有的自然数编码意义不大，不如将所有频数为1的类别合并到同一个新的类别下(e.g. rare ones)。注意：如果特征列的频数需要被当做一个新的特征加入数据集，请在上述合并之前提取出频数特征。
        - Histogram映射：提取某些特定的特征列，根据label内容做统计，把label中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来.
         >统计“性别与爱好的关系”，**性别**有“男”、“女”. **爱好**有三种，表示成向量 [散步、足球、看电视剧]，分别计算男性和女性中每个爱好的比例得到：男[1/3, 2/3, 0]，女[0, 1/3, 2/3]。即反映了两个特征的关系。
     - 时间特征
         - 持续时间(单页浏览时长)/间隔时间(上次购买/点击离现在的时间)
         - 离散时间
            - 一天中哪个时间段(hour_0-23)/一周中星期几(week_monday...)/一年中哪个星期/ 一年中哪个季度/工作日/周末数据挖掘中经常会用时间作为重要特征，比如电商可以分析节假日和购物的关系，一天中用户喜好的购物时间等。
     
     - 文本特征
         - 词袋：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋．
         - 把词袋中的词扩充到n-gram
         - 使用TF-IDF特征
     
     - 统计特征
         - 加减平均：商品价格高于平均价格多少，用户在某个品类下消费超过平均用户多少，用户连续登录天数超过平均多少...
         - 分位线：商品属于售出商品价格的多少分位线处
         - 次序型：排在第几位
         - 比例类：电商中，好/中/差评比例，你已超过全国百分之…的同学
     - 组合特征
         -  拼接型：简单的组合特征。例如挖掘用户对某种类型的喜爱，对用户和类型做拼接。正负权重，代表喜欢或不喜欢某种类型。e.g. user_id&&category: 10001&&女裙 10002&&男士牛仔
         -  模型特征组合：用GBDT产出特征组合路径,组合特征和原始特征一起放进LR训练
      


 
3. 建立新特征
   特征之间也可以进行操作来合成新特征,可能会产生更好效果:
   - 单独特征列乘以一个常数（constant multiplication）或者加减一个常数：对于创造新的有用特征毫无用处；只能作为对已有特征的处理。
   - 任何针对单独特征列的单调变换（如对数）：不适用于决策树类算法。对于决策树而言https://zhuanlan.zhihu.com/p/26444240
   - 线性组合（linear combination）：仅适用于决策树以及基于决策树的ensemble（如gradient boosting, random forest），因为常见的axis-aligned split function不擅长捕获不同特征之间的相关性；不适用于SVM、线性回归、神经网络等。
   - 比例特征（ratio feature）: 特征1/特质2 = 特征3 ...  绝对值/max/min...
   - 同时利用pandas的groupby操作结合类别与数值特征, e.g.　mean(身高)group_by籍贯...  将这种方法和线性组合等基础特征工程方法结合（仅用于决策树），可以得到更多有意义的特征
   - 提取决策树中生成的向量作为新特征:
      - 在决策树系列的算法中（单棵决策树、gbdt、随机森林），每一个样本都会被映射到决策树的一片叶子上。因此，我们可以把样本经过每一棵决策树映射后的index（自然数）或one-hot-vector（哑编码得到的稀疏矢量）作为一项新的特征，加入到模型中。具体实现：apply()以及decision_path()方法，在scikit-learn和xgboost里都可以用。
   - 树类模型的ensemble: spearman correlation coefficient    /   线性模型、SVM、神经网络: 对数（log）,pearson correlation coefficient

4.Data leakage issues 数据泄露问题
   - https://zhuanlan.zhihu.com/p/26444240
 
5. 特征选择
   
   特征选择和降维有什么区别呢？特征选择只踢掉原本特征里和结果预测关系不大的， 降维做特征的计算组合构成新特征。当做完特征转换后，实际上可能会存在很多的特征属性，比如：多项式扩展转换、文本数据转换等等，但是太多的特征属性的存在可能会导致模型构建效率降低，同时模型的效果有可能会变的不好，那么这个时候就需要从这些特征属性中选择出影响最大的特征属性作为最后构建模型的特征属性列表。https://www.cnblogs.com/massquantity/p/10486904.html
   - sklearn.feature_selection
   - 特征选择的**2个方面**
      - 特征是否发散：如果一个特征不发散，比如方差接近于0，也就是说这样的特征对于样本的区分没有什么作用。
      - 特征与目标的相关性：如果与目标相关性比较高，应当优先选择。
   - 特征选择的 **3个方法** 
      - Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，从而选择特征；常用方法包括方差选择法、相关系数法、卡方检验、互信息法等。其优点是计算快，不依赖于具体的模型，缺点是选择的统计指标不是为特定模型定制的，因而最后的准确率可能不高。而且因为进行的是单变量统计检验，没有考虑特征间的相互关系。
         - 缺失样本比例过多且难以填补的特征，建议剔除该变量。
         - 若某连续型变量的方差接近于0，说明其特征值趋向于单一值的状态，对模型帮助不大，建议剔除该变量。
         - 若某类别型变量的枚举值样本量占比分布，集中在单一某枚举值上，建议剔除该变量。
      - Wrapper：包装法，使用模型来筛选特征，通过不断地增加或删除特征，在验证集上测试模型准确率，寻找最优的特征子集。包裹式方法因为有模型的直接参与，因而通常准确性较高，但是因为每变动一个特征都要重新训练模型，因而计算开销大，其另一个缺点是容易过拟合。方法：把特征选择看做一个特征子集搜索问题， 筛选各种特征子集， 用模型评估效果
        https://blog.csdn.net/qq_28611929/article/details/89676692 给出了一些例子
        https://www.jianshu.com/p/48da5a502398 例子2
      

      - Embedded：嵌入法，利用了模型本身的特性，将特征选择嵌入到模型的构建过程中。典型的如 Lasso 和树模型等。准确率较高，计算复杂度介于过滤式和包裹式方法之间，但缺点是只有部分模型有这个功能。


6. 集成学习Ensemble Method
   - 集成学习方法是指组合多个模型，以获得更好的效果，使集成的模型具有更强的泛化能力。Ensemble Learning 是指将多个不同的 Base Model 组合成一个 Ensemble Model 的方法。它可以同时降低最终模型的 Bias 和 Variance从而在提高分数的同时又降低 Overfitting 的风险。在现在的 Kaggle 比赛中要不用 Ensemble 就拿到奖金几乎是不可能的。
      - 在验证集上找表现最好的模型
      - 强可学习和弱可学习
      - 多个模型投票或者取平均值
      - bagging:Boostrap Aggregating,  在 Bagging 方法中，让学习算法训练多次，每次的训练集由初始的训练集中随机取出的个训练样本组成，初始的训练样本在某次的训练集中可能出现多次或者根本不出现。最终训练出个预测函数. 对于分类和回归问题可采用如下的两种方法:分类问题：采用投票的方法，得票最多的类别为最终的类别/回归问题：采用简单的平均方法. 随机森林算法就是基于 Bagging 思想的学习算法。
      - boosting: 在 Boosting 算法中，初始化时对每个训练样本赋予相等的权重,然后用该学习算法对训练集训练几轮，每次训练后，对训练失败的训练样本赋予更大的权重，也就是让学习算法在后续的学习中几种对比较难学的训练样本进行学习，从而得到一个预测函数序列,其中每个 都有一个权重，预测效果好的预测函数的权重较大。最终的预测函数为对于分类和回归问题可采用如下的两种方法：分类问题：有权重的投票方式/回归问题：加权平均. AdaBoost 和 GBDT(Gradient Boosting Decision Tree) 是基于 Boosting 思想的两个最著名的算法。比 Bagging 效果好，但更容易 Overfit。
      - Blending：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。
      - Stacking: 相比 Blending，Stacking 能更好地利用训练数据。相比 Blending，Stacking 能更好地利用训练数据。以 5-Fold Stacking 为例，整个过程很像 Cross Validation。首先将训练数据分为 5 份，接下来一共 5 个迭代，每次迭代时，将 4 份数据作为 Training Set 对每个 Base Model 进行训练，然后在剩下一份 Hold-out Set 上进行预测。同时也要将其在测试数据上的预测保存下来。这样，每个 Base Model 在每次迭代时会对训练数据的其中 1 份做出预测，对测试数据的全部做出预测。5 个迭代都完成以后我们就获得了一个 #训练数据行数 x #Base Model 数量 的矩阵，这个矩阵接下来就作为第二层的 Model 的训练数据。当第二层的 Model 训练完以后，将之前保存的 Base Model 对测试数据的预测（因为每个 Base Model 被训练了 5 次，对测试数据的全体做了 5 次预测，所以对这 5 次求一个平均值，从而得到一个形状与第二层训练数据相同的矩阵）拿出来让它进行预测，就得到最后的输出。
      - 从理论上讲，Ensemble 要成功，有两个要素：
         - Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。
         - Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。
         

7. 参考资料
   - https://developer.aliyun.com/article/857312?spm=5176.26934562.main.8.607b5c00iLaDLZ 一个发电厂收入分析的例子, 比较详细的给出了从头到尾的各种决策及代码样例, 非常有价值
   - https://mp.weixin.qq.com/s/AfSG23U76N3OT7iElLPQTQ   特征选择的一篇总结性博文, 结构性的总结了所有基本的特征选择的概念/理论
   - https://www.cnblogs.com/jasonfreak/p/5448385.html   同样的一篇总结博文,非常实用
9.
10.
11.
12.











### 训练集/测试集 合并问题
