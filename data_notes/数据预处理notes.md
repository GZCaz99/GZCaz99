# 机器学习项目的数据处理笔记
在建模前,需要对数据集进行处理,这也是所有学习项目(分类/回归)中最重要且一般耗时最多的部分. 一般来说,每个项目都要求我们根据若干个数据集中提取数据进行建模,常见工作流程如下:
   
   1. 观察数据集,看看是否有重复的特征列,以及各特征列的分布/类型等(EDA阶段)
        - df.head()/df.info()/df.describe()/df.display()..
        - 观察各特征列的类型
        - 观察缺失值情况,选择合适的填充方法或者直接去掉
            - scikitlearn 中的 simple imputer 包
            - xgboost 中自动处理
            - 直接删去(如果占比不多可以考虑,如果某一列的缺失值太多则考虑直接去掉)
            - 异常值(outlier), 观察最大/最小值, 3 $\sigma$ 原则, 箱型图、直方图（尾巴）、散点图（常用）, 方差（离目标远近程度，对噪声敏感）、分位图（0.5%~99.5%）四分位数间距：25%~75%之间的区间宽度。

        - 观察各个特征列中值的分布情况
            - 核密度估计: seaborn.violinplot/histplot...
            - correlation, 热点图
            - 观察label的分布情况: 如果df1中每个x对应一个label, df2中又对应另一个label.../根据项目中不同数据集的特点想办法合并成为一个训练集

        - 数据预处理
            - from sklearn.preprocessing import ...
            - 1. 数据取值范围缩放: 数据标准化（Standardization）用到的多,数据归一化（Scaling）用的少,数据正规化（Normalization）

        
  
  2. 特征工程
     - 特征编码
          - polynomial: 它是使用多项式的方法来进行的，如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2）https://blog.csdn.net/xiaohutong1991/article/details/107945459
          
     - 数值型特征:
          - 做对数log变换(train.SalePrice = np.loglp(train.SalePrice)) , 多项式扩展数值特征
            其实就是多项式编码
            
     - 分类特征:
        - 二值化(binomial),用0/1代表不同类(e.g, iris 是个三分类数据集,但可以用1来表示第一和第二类,用2来表示第三类), 对于同时属于多类的label,可以参考
        - label encoder, 用有序数据对**不连续**的label 编号, e.g. iris中的花中类, vertosa 设为 3, sentosa 设为 2 ...
        - one-hot encoding, 一般是多类label/类之间没有顺序联系时用, 在数据集中创建一个新的df,有多少类这个数据就有多少维, e.g. 有6类数据, 200行, 维度就是6*200. 
        - meanencoding, 针对高基数类别特征的有监督编码,针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。优点：和独热编码相比，节省内存、减少算法计算时间、有效增强模型表现。
        - 在类别特征列里，有时会有一些类别，在训练集和测试集中总共只出现一次，例如特别偏僻的郊区地址。此时，保留其原有的自然数编码意义不大，不如将所有频数为1的类别合并到同一个新的类别下(e.g. rare ones)。注意：如果特征列的频数需要被当做一个新的特征加入数据集，请在上述合并之前提取出频数特征。
 3. 












### 训练集/测试集 合并问题
