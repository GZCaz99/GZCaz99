# 机器学习项目的数据处理笔记
在建模前,需要对数据集进行处理,这也是所有学习项目(分类/回归)中最重要且一般耗时最多的部分. 一般来说,每个项目都要求我们根据若干个数据集中提取数据进行建模,常见工作流程如下:
   
   1. 观察数据集,看看是否有重复的特征列,以及各特征列的分布/类型等(EDA阶段)
        - df.head()/df.info()/df.describe()/df.display()..
        - 观察各特征列的类型
        - 观察缺失值情况,选择合适的填充方法或者直接去掉
            - scikitlearn 中的 simple imputer 包
               > https://cloud.tencent.com/developer/article/1786924
            - LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。
            - 直接删去(如果占比不多可以考虑,如果某一列的缺失值太多则考虑直接去掉)
            - 异常值(outlier), 观察最大/最小值, 3 $\sigma$ 原则, 箱型图、直方图（尾巴）、散点图（常用）, 方差（离目标远近程度，对噪声敏感）、分位图（0.5%~99.5%）四分位数间距：25%~75%之间的区间宽度。

        - 观察各个特征列中值的分布情况
            - 核密度估计: seaborn.violinplot/histplot...
            - correlation, 热点图
            - 观察label的分布情况: 如果df1中每个x对应一个label, df2中又对应另一个label.../根据项目中不同数据集的特点想办法合并成为一个训练集
            - 对 Numerical Variable，可以用 Box Plot 来直观地查看它的分布。对于坐标类数据，可以用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在。对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。

        - 数据预处理
            - from sklearn.preprocessing import ...
            - 数据取值范围缩放: 数据标准化（Standardization）用到的多,数据归一化（Scaling）用的少,数据正规化（Normalization）
            - Standardization: sklearn.preprocessing.StandardScaler,转换为Z-score，使数值特征列的算数平均为0，方差（以及标准差）为1。不免疫outlier。或者 如果数值特征列中存在数值极大或极小的outlier（通过EDA发现），应该使用更稳健（robust）的统计数据：用中位数而不是算术平均数，用分位数（quantile）而不是方差。这种标准化方法有一个重要的参数：（分位数下限，分位数上限），最好通过EDA的数据可视化确定。免疫outlier。(sklearn.preprocessing.RobustScaler)
            - scaling:将一列的数值，除以这一列的最大绝对值。不免疫outlier。
               -  sklearn.preprocessing.MaxAbsScaler
               -  preprocessing.MinMaxScaler, 将属性缩放到一个指定的最大值和最小值(通常是1-0)之间,对于方差非常小的属性可以增强其稳定性
            - normalization归一化: sklearn.preprocessing.Normalizer 奇异样本数据的存在会引起训练时间增大，同时也可能导致无法收敛，因此，当存在奇异样本数据时，在进行训练之前需要对预处理数据进行归一化；反之，不存在奇异样本数据时，则可以不进行归一化。
               - 简而言之，归一化的**目的**就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。归一化的**过程**是将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性这个方法会很有用. Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础.Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。https://zhuanlan.zhihu.com/p/424518359
               - 或者在特征数据取值范围相差太大时,也需要运用归一化. e.g. 房价(y), 房间数(0-10),面积（20-1000）
               - 加快了梯度下降求最优解的速度/有可能提高精度
               - 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、gbdt、xgboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。
         - 数据采样：采集、清洗过数据以后，正负样本可能是不均衡的，要进行数据采样。采样的方法有随机采样和分层抽样。但是随机采样会有隐患，因为可能某次随机采样得到的数据很不均匀，更多的是根据特征采用分层抽样。
            - 分层采样
            -  正负样本不平衡处理办法：
               >正样本 >> 负样本，且量都挺大 => downsampling（下采样，去除一些正例，使得正例和反例数量接近）
               >正样本 >> 负样本，量不大 => 采集更多的数据/上采样/oversampling(比如图像识别中的镜像和旋转) -增加反例使得正例和反例数量接近/修改损失函数/loss function (设置样本权重)

                
        
  
  2. 特征工程

     - 特征编码
          - polynomial: 它是使用多项式的方法来进行的，如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2）https://blog.csdn.net/xiaohutong1991/article/details/107945459
          
     - 数值型特征:
          - 做对数log变换(train.SalePrice = np.loglp(train.SalePrice)) , 多项式扩展数值特征
            其实就是多项式编码
         - 离散化：把连续值转成非线性数据。例如电商会有各种连续的价格表，从0.03到100元，假如以一元钱的间距分割成99个区间，用99维的向量代表每一个价格所处的区间，1.2元和1.6元的向量都是 [0,1,0,…,0]。pd.cut() 可以直接把数据分成若干段
         - 观察柱状分布：离散化后统计每个区间的个数做柱状图。
            
     - 分类特征:
        - 二值化(binomial),用0/1代表不同类(e.g, iris 是个三分类数据集,但可以用1来表示第一和第二类,用2来表示第三类), 对于同时属于多类的label,可以参考
        - label encoder, 用有序数据对**不连续**的label 编号, e.g. iris中的花中类, vertosa 设为 3, sentosa 设为 2 ...
        - one-hot encoding, 一般是多类label/类之间没有顺序联系时用, 在数据集中创建一个新的df,有多少类这个数据就有多少维, e.g. 有6类数据, 200行, 维度就是6*200. 
        - meanencoding, 针对高基数类别特征的有监督编码,针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。优点：和独热编码相比，节省内存、减少算法计算时间、有效增强模型表现。
        - 在类别特征列里，有时会有一些类别，在训练集和测试集中总共只出现一次，例如特别偏僻的郊区地址。此时保留其原有的自然数编码意义不大，不如将所有频数为1的类别合并到同一个新的类别下(e.g. rare ones)。注意：如果特征列的频数需要被当做一个新的特征加入数据集，请在上述合并之前提取出频数特征。
        - Histogram映射：提取某些特定的特征列，根据label内容做统计，把label中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来.
         >统计“性别与爱好的关系”，**性别**有“男”、“女”. **爱好**有三种，表示成向量 [散步、足球、看电视剧]，分别计算男性和女性中每个爱好的比例得到：男[1/3, 2/3, 0]，女[0, 1/3, 2/3]。即反映了两个特征的关系。
     - 时间特征
         - 持续时间(单页浏览时长)/间隔时间(上次购买/点击离现在的时间)
         - 离散时间
            - 一天中哪个时间段(hour_0-23)/一周中星期几(week_monday...)/一年中哪个星期/ 一年中哪个季度/工作日/周末数据挖掘中经常会用时间作为重要特征，比如电商可以分析节假日和购物的关系，一天中用户喜好的购物时间等。
     
     - 文本特征
         - 词袋：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋．
         - 把词袋中的词扩充到n-gram
         - 使用TF-IDF特征
     
     - 统计特征
         - 加减平均：商品价格高于平均价格多少，用户在某个品类下消费超过平均用户多少，用户连续登录天数超过平均多少...
         - 分位线：商品属于售出商品价格的多少分位线处
         - 次序型：排在第几位
         - 比例类：电商中，好/中/差评比例，你已超过全国百分之…的同学
     - 组合特征
         -  拼接型：简单的组合特征。例如挖掘用户对某种类型的喜爱，对用户和类型做拼接。正负权重，代表喜欢或不喜欢某种类型。e.g. user_id&&category: 10001&&女裙 10002&&男士牛仔
         -  模型特征组合：用GBDT产出特征组合路径,组合特征和原始特征一起放进LR训练
      


 
3. 建立新特征
   特征之间也可以进行操作来合成新特征,可能会产生更好效果:
   - 单独特征列乘以一个常数（constant multiplication）或者加减一个常数：对于创造新的有用特征毫无用处；只能作为对已有特征的处理。
   - 任何针对单独特征列的单调变换（如对数）：不适用于决策树类算法。对于决策树而言https://zhuanlan.zhihu.com/p/26444240
   - 线性组合（linear combination）：仅适用于决策树以及基于决策树的ensemble（如gradient boosting, random forest），因为常见的axis-aligned split function不擅长捕获不同特征之间的相关性；不适用于SVM、线性回归、神经网络等。
   - 比例特征（ratio feature）: 特征1/特质2 = 特征3 ...  绝对值/max/min...
   - 同时利用pandas的groupby操作结合类别与数值特征, e.g.　mean(身高)group_by籍贯...  将这种方法和线性组合等基础特征工程方法结合（仅用于决策树），可以得到更多有意义的特征
   - 提取决策树中生成的向量作为新特征:
      - 在决策树系列的算法中（单棵决策树、gbdt、随机森林），每一个样本都会被映射到决策树的一片叶子上。因此，我们可以把样本经过每一棵决策树映射后的index（自然数）或one-hot-vector（哑编码得到的稀疏矢量）作为一项新的特征，加入到模型中。具体实现：apply()以及decision_path()方法，在scikit-learn和xgboost里都可以用。
   - 树类模型的ensemble: spearman correlation coefficient    /   线性模型、SVM、神经网络: 对数（log）,pearson correlation coefficient

4.Data leakage issues 数据泄露问题
   - https://zhuanlan.zhihu.com/p/26444240
 
5. 特征选择
   特征选择和降维有什么区别呢？特征选择只踢掉原本特征里和结果预测关系不大的， 降维做特征的计算组合构成新特征。当做完特征转换后，实际上可能会存在很多的特征属性，比如：多项式扩展转换、文本数据转换等等，但是太多的特征属性的存在可能会导致模型构建效率降低，同时模型的效果有可能会变的不好，那么这个时候就需要从这些特征属性中选择出影响最大的特征属性作为最后构建模型的特征属性列表。
   - 特征选择的**2个方面**
      - 特征是否发散：如果一个特征不发散，比如方差接近于0，也就是说这样的特征对于样本的区分没有什么作用。
      - 特征与目标的相关性：如果与目标相关性比较高，应当优先选择。
   - 特征选择的 **3个方法** 
      - Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，从而选择特征；常用方法包括方差选择法、相关系数法、卡方检验、互信息法等。
      - Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征或者排除若干特征；常用方法主要是递归特征消除法。
      - Embedded：嵌入法，先使用某些机器学习的算法和模型。


6.
7.
8.
9.
10.
11.











### 训练集/测试集 合并问题
