{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#手撕multi-attention机制\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import math\n",
    "\n",
    "#初始化输入\n",
    "x  = torch.randn(128,64,512)\n",
    "#定义模型参数\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, q, k, v,mask = None):\n",
    "        batch, time, dimension = q.shape\n",
    "        n_dim = self.d_model // self.n_heads\n",
    "        q,k,v = self.wq(q), self.wk(k), self.wv(v)\n",
    "        q = q.view(batch,time,self.n_heads,n_dim).permute(0,2,1,3)\n",
    "        k = k.view(batch,time,self.n_heads,n_dim).permute(0,2,1,3)\n",
    "        v = v.view(batch,time,self.n_heads,n_dim).permute(0,2,1,3)\n",
    "\n",
    "        attn_score = q@k.transpose(2,3) / math.sqrt(n_dim)\n",
    "        if mask is not None:\n",
    "            # mask = torch.tril(torch.ones(time,time,d_tpye =bool))\n",
    "            attn_score = attn_score.masked_fill(mask == 0, -1e9)\n",
    "        attn_score = self.softmax(attn_score) @ v\n",
    "        #contigeous()是为了让attn_score在内存中也是连续的,这样才可以使用view()\n",
    "        attn_score = attn_score.permute(0,2,1,3).contiguous().view(batch,time,dimension)\n",
    "        output = self.w_combine(attn_score)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "#encoder层\n",
    "#首先是token embedding\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) \n",
    "\n",
    "#然后是位置编\n",
    "# 提供位置信息：在自然语言处理（NLP）等序列处理任务中，单词的顺序通常是语义理解的关键。位置编码确保模型能够捕捉到这种顺序关系，即使自注意力层本身并不直接处理位置信息。\n",
    "# 保持模型的并行性：在Transformer模型中，自注意力层和前馈网络层可以并行处理序列中的所有元素。位置编码允许模型在保持这种并行性的同时，理解元素之间的位置关系。\n",
    "# 增强模型的表达能力：通过将位置编码添加到输入表示中，模型可以学习到位置相关的特征，这有助于提高模型对序列数据的理解深度。\n",
    "# 允许模型处理可变长度的序列：位置编码使得模型能够处理不同长度的输入序列。通过为每个可能的位置提供唯一的编码，模型可以适应不同长度的输入。\n",
    "# 训练效率：位置编码是固定的或可学习的参数，这意味着它们不需要在每个训练步骤中重新计算。这有助于提高训练效率，尤其是在处理长序列时。\n",
    "class positionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len,device):\n",
    "        super(positionalEmbedding, self).__init__()\n",
    "        #初始化位置编码,他是一个全0矩阵，大小为max_len*d_model\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.require_grad(False)\n",
    "        #创建一个从0到max_len-1的序列，形状为(max_len, 1)。\n",
    "        pos = torch.arange(0,max_len,device=device).float().unsqueeze(1)\n",
    "        #创建一个序列，包含从0开始到d_model-1的步长为2的整数，用于计算正弦和余弦函数的频率。\n",
    "        _2i = torch.arange(0,d_model,step=2,device=device).float()\n",
    "        #使用sin和cos函数生成位置编码。对于d_model中的每个维度，如果索引是偶数，则使用sin函数；如果索引是奇数，则使用cos函数。这样做是为了让模型能够从位置编码中学习到位置信息，同时保持模型的对称性。\n",
    "        self.encoding[:,0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:,1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        #返回位置编码矩阵的前seq_len行，以匹配输入序列的长度。\n",
    "        return self.encoding[:seq_len,:]\n",
    "    \n",
    "\n",
    "#然后是layer norm\n",
    "#减少对显存的需求\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        class layer_norm(nn.Module):\n",
    "            def __init__(self, d_model, eps=1e-6):\n",
    "                super(layer_norm, self).__init__()\n",
    "                self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "                self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "                self.eps = eps\n",
    "            def forward(self, x):\n",
    "                mean = x.mean(-1, keepdim=True)\n",
    "                var = x.var(-1.unbiased=False, keepdim=True)\n",
    "                out = (x - mean)/torch.sqrt(var + self.eps)\n",
    "                out = self.gamma * out + self.beta\n",
    "                return out\n",
    "            \n",
    "#然后是全连接层\n",
    "#position-wise feed forward network\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model,hidden,dropout=0.1):\n",
    "        self.fc1 = nn.Linear(d_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "#拼在一起形成transformer的embedding:\n",
    "#输出是attention层的输入\n",
    "class transformerEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_len, device,drop_prop):\n",
    "        super(transformerEmbedding, self).__init__():\n",
    "        self.tokemb = TokenEmbedding(d_model, vocab_size)\n",
    "        self.posemb = positionalEmbedding(d_model, max_len, device)\n",
    "        self.drop = nn.Dropout(drop_prop)\n",
    "    def forward(self, x):\n",
    "        tokemb = self.tokemb(x)\n",
    "        posemb = self.posemb(x)\n",
    "        return self.drop_out(tokemb + posemb)\n",
    "        \n",
    "\n",
    "#最后就是encoder层\n",
    "#self-attention > add & norm > position-wise feed forward > add & norm\n",
    "class encoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, hidden, dropout=0.1):\n",
    "        super(encoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, hidden, dropout)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        _x = x\n",
    "        x = self.self_attn(x, x, x, mask)\n",
    "        x = self.drop1(x)\n",
    "        x = self.layer_norm1(x + _x)\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        self.drop2(x)\n",
    "        s  = self.layer_norm2(x + _x)\n",
    "        return s\n",
    "\n",
    "\n",
    "#decoder层\n",
    "#masked multi-head attention > add & norm > cross attention > add & norm > position-wise feed forward > add & norm\n",
    "class decoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, hidden, dropout=0.1,mask=True):\n",
    "        super(decoderLayer, self).__init__()\n",
    "        self.attn1 = MultiHeadAttention(d_model, n_heads)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads,mask)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, hidden, dropout)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "    #加入两个特殊mask,一个是decoder的mask,一个是encoder的mask\n",
    "    #t_mask 又叫做下三角掩码,用来mask未来序列的信息\n",
    "    #s_mask用来忽视掉 padding的信息\n",
    "    def forward(self,dec,enc,t__mask,s_mask):    \n",
    "        _x = dec\n",
    "        x = self.attn1(dec, dec, dec, t__mask)\n",
    "        x = self.drop1(x)\n",
    "        x = self.layer_norm1(x + _x)\n",
    "        _x = x\n",
    "        if enc is not None:\n",
    "            _x = x\n",
    "            x = self.cross_attn(x, enc, enc, s_mask)\n",
    "            x = self.drop2(x)\n",
    "            x = self.layer_norm2(x + _x)\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.layer_norm3(x + _x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#一整个encoders有多个encoder layer组成\n",
    "class transformerEncoder(nn.Module):\n",
    "    def __init__(self,enc_voc_size,max_len,d_model,ffn_hidden,n_heads,n_layers,dropout=0.1,device):\n",
    "        super(transformerEncoder, self).__init__()\n",
    "        self.embedding = transformerEmbedding(d_model, enc_voc_size, max_len, device, dropout)\n",
    "        self.layers = nn.ModuleList([encoderLayer(d_model, n_heads, ffn_hidden, dropout) for _ in range(n_layers)])\n",
    "    def forward(self, x, s_mask):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "#decoder层\n",
    "class transformerDecoder(nn.Module):\n",
    "    def __init__(self,dec_voc_size,max_len,d_model,ffn_hidden,n_heads,n_layers,dropout=0.1,device):\n",
    "        super(transformerDecoder, self).__init__():\n",
    "        self.embedding = transformerEmbedding(d_model, dec_voc_size, max_len, device, dropout)\n",
    "        self.layers = nn.ModuleList([decoderLayer(d_model, n_heads, ffn_hidden, dropout) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, dec_voc_size)\n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        dec = self.embedding(dec)\n",
    "        for layer in self.layers:    \n",
    "            x = layer(dec, enc, t_mask, s_mask)\n",
    "        dec = self.fc(dec)\n",
    "        return x\n",
    "\n",
    "\n",
    "#最后就是transformer\n",
    "class transformer(nn.Module):\n",
    "    def __init__(self,src_pad_idx,trg_pad_idx,enc_voc_size,dec_voc_size,max_len,d_model,ffn_hidden,n_heads,n_layers,dropout=0.1,device):\n",
    "        super(transformer, self).__init__()\n",
    "        self.encoder = transformerEncoder(enc_voc_size,max_len,d_model,ffn_hidden,n_heads,n_layers,dropout,device)\n",
    "        self.decoder = transformerDecoder(dec_voc_size,max_len,d_model,ffn_hidden,n_heads,n_layers,dropout,device)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    #建立t-mask方法\n",
    "    def make_trg_mask(self, q,k):\n",
    "        len_q,len_k = q.size(1),k.size(1)\n",
    "        mask = torch.tril(torch.ones(len_q,len_k,device=self.device)).bool()\n",
    "        return mask\n",
    "    #建立s-mask方法\n",
    "    def make_src_mask(self, q,k,pad_idx_q,pad_idx_k):\n",
    "        len_q,len_k = q.size(1),k.size(1)\n",
    "        #(batch,time,len_q,len_k)\n",
    "        q = q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1,1,1,len_k)\n",
    "\n",
    "        k = k.ne(pad_idx_k).unsqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1,1,len_q,1)\n",
    "        #与运算,得到s_mask\n",
    "        mask = q & k\n",
    "        return mask\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src,src,self.src_pad_idx,self.src_pad_idx)\n",
    "        trg_mask = self.make_trg_mask(trg,trg,self.trg_pad_idx,self.trg_pad_idx) * self.make_trg_mask(trg,trg,self.src_pad_idx,self.src_pad_idx)\n",
    "        src_trg_mask = self.make_src_mask(trg,src,self.trg_pad_idx,self.src_pad_idx)\n",
    "        enc = self.encoder(src, src_mask)\n",
    "        dec = self.decoder(trg, enc, trg_mask, src_trg_mask)\n",
    "        return dec\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
