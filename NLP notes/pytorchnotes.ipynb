{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#神经网络\n",
    "#神经网络由一个个神经元组成,每个神经元的作用是计算并向前输出结果\n",
    "#即,给定一个输入向量(因为多个神经元互相连接,而每个神经元最后的输出为一个标量),输出标量\n",
    "#设定 't' 为输出标量, 'f' 为定义的激活函数, 'wi' 为第i个输入向量元素的权重, 'ai' 为输入向量中的第i个元素, b为截距项\n",
    "#这样就可以表达为 每个神经元的输出t = f(W^T * A + b), 这里 W^T 是整个 输入向量的 权重的 transfer matrix, A 是输入向量\n",
    "\n",
    "\n",
    "#single layer neural network\n",
    "#单层网络是最基本的网络形式,由n个神经元组冲, 而每个神经元的输入向量都是同一个\n",
    "#输出也为一个n维向量\n",
    "\n",
    "\n",
    "#感知机 perceptron\n",
    "#由输入 输出两层组成,输入层处理后传递给输出层,输出(+/- 1)\n",
    "#简单的二分类模型,通过定义阈值分类数据\n",
    "\n",
    "\n",
    "#多层神经网络\n",
    "#输入-n个隐藏-输出层, 隐藏层越多,神经网络的robustness越好\n",
    "#内部还能细分为全连接层, 即这一层和前一层的每一个神经元都互相连接\n",
    "#如果第n层是全连接层, 那它本质上只是对第n-1 层计算了一次 Y = Wx+b 的计算, Y可以理解为第n层的输出, x可以理解为上一层的输出, W还是权重向量\n",
    "\n",
    "\n",
    "#激活函数\n",
    "#激活函数能够极大的优化分类/回归的结果\n",
    "#提高模型鲁棒性,缓解梯度消失/加速模型收敛\n",
    "\n",
    "\n",
    "#梯度下降\n",
    "#在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度\n",
    "#梯度是一个向量, 是不同节点间导数变化最快的一条向量(学习路径),他的意义从几何意义上讲，就是函数变化增加最快的地方\n",
    "#沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向 梯度减少最快，也就是更加容易找到函数的最小值。\n",
    "#梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解\n",
    "#分类(log损失)和回归任务(平方损失)中都有损失,我们的任务就是不断地调整 W 来减少损失\n",
    "#过程就是算出导数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch notes\n",
    "#pytorch 用于建立神经网络,可以看成是一个更高级的numpy\n",
    "import torch\n",
    "\n",
    "#创建空矩阵\n",
    "a = torch.empty(x,y)\n",
    "#创建给定size的随机填充矩阵\n",
    "a = torch.rand(x,y) #取值区间[0,1) \n",
    "a = torch.randint(x,y,(a,b))    #随机整数,取值范围(a-b) \n",
    "#创建全0矩阵\n",
    "a = torch.zeros(x,y)\n",
    "#直接通过数据创建张量\n",
    "a = torch.tensor([....].[....],...)\n",
    "#更新张量数据\n",
    "a = a.new_...(x,y)  #new_后面可接各类方法,详情看文档\n",
    "a = torch.xxx_like(a)   #xxx为各类方法,同样看文档\n",
    "a.type                  #查看数据类型\n",
    "#所有上面的方法都可以在括号中输入 dtype = '...' 来指定创建数据的类型,e.g torch.float..\n",
    "#张量size:\n",
    "print(a.size())     #输出为数组,可以切片,\n",
    "print(a.dim())      #查看维度,输出为一个数\n",
    "a.transpose(a,b)        #交换张量维度(2维)\n",
    "a.permute(a,b,c )             #交换张量维度(3维)\n",
    "a.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch 基本运算\n",
    "#加法\n",
    "print(a + b)\n",
    "print(torch.add(a,b))   #常用的两种,结果一样\n",
    "\n",
    "result - torch.empty(x,y)\n",
    "torch.add(x,y, out = result)\n",
    "print(result)       #需要直接储存结果的时候,也比较常用\n",
    "\n",
    "b.add_(a)\n",
    "print(b)            #第四种方法,原地操作,其实是直接改变b的值,所有原地操作的方法都会在后面加下划线,比如 copy_/sub_ ...\n",
    "\n",
    "#切片\n",
    "a[x,y,...]      #输出a种第x行第y列...的数据\n",
    "a[x,y,...] = n  #改变第x行第y列...的值\n",
    "#统计\n",
    "a.std()\n",
    "a.mean()\n",
    "\n",
    "#切片, pytorch中切片和numpy中一样,比如\n",
    "print(a[:,:2]) #表示选中所有行,到第二列\n",
    "\n",
    "#改变张量的形状 \n",
    "torch.unsqueeze(a,int)  #在原张量size的第int位插入一维,int 的取值范围要在(-a.dim()-1,a.dim()+1)之内,否则会报错\n",
    "b = a.view()    #tensor.view()  需要保证元素的总数量不变\n",
    "b = a.view(-1,n)    #将原张量分成n份, -1表示自动匹配个数(或 (n, -1))\n",
    "a.item()        #当张量内只有一个元素时,该方法提取它,并输出为一个数字(很常用)\n",
    "\n",
    "#tensor转换成array\n",
    "a.numpy()   #require_grad = True 时不能直接转换, 要使用:\n",
    "a.detach().numpy()\n",
    "#array转换成tensor\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "#所有在cpu上的tensor都可以转换为array,除了char类型的\n",
    "#如果tensor储存在gpu上,则:\n",
    "if torch.cuda.is_avaliable():\n",
    "    device = torch.device('cuda')\n",
    "    a = torch.ones_like(x,y. device=device)\n",
    "    b = a.to(device)           #将array转移后在转换成张量\n",
    "\n",
    "# 一般张量都放在gpu上运行,cpu留着做其他事\n",
    "if torch.cuda.is_avaliable():\n",
    "    tensor = tensor.to('cuda')\n",
    "\n",
    "# tensor可以concat,dim = 1为竖向拼接(拼到下面),0为横向拼接(cat[[0,1,2],[3,5,6]] = [0,1,2,3,5,6]):\n",
    "concat_tensor = torch.cat([tensor1,tensor2,ensor3,...],dim = 1/0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "None <AddBackward0 object at 0x000001D2E9DD1720>\n"
     ]
    }
   ],
   "source": [
    "#torch.Tensor\n",
    "#建立张量时可以指定属性 requires_grad = true  来储存跟踪在张量上的所有处理,并且自动求导\n",
    "#除此之外还可以保留一个属性 grad_fn, 输出为张量的处理方法(运算+传播方向)\n",
    "a = torch.ones(2,2,requires_grad= True)\n",
    "a.requires_grad_(True)  #当建立时没有指定,可以通过该方法原地指定属性\n",
    "print(a)\n",
    "b = a + 2\n",
    "print(b)\n",
    "print(a.grad_fn , b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#梯度相关\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#pytorch反向传播的方法是 .backward(),输出张量的求导结果(当张量的输出是一个标量时)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mb\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(b\u001b[38;5;241m.\u001b[39mgrad)   \u001b[38;5;66;03m#输出张量梯度(当a = x 时求导结果)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mbackard())   \u001b[38;5;66;03m#根据损失函数对参数计算梯度\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "#梯度相关\n",
    "#pytorch反向传播的方法是 .backward(),输出张量的求导结果(当张量的输出是一个标量时)\n",
    "b.backward()\n",
    "print(b.grad)   #输出张量梯度(当a = x 时求导结果)\n",
    "print(loss.backard())   #根据损失函数对参数计算梯度\n",
    "b.data  #输出张量此时中的数据\n",
    "#输出不求导的张量(停止追踪张量上的操作),在评估模型时用, 可以极大地减少内存占用\n",
    "with torch.no_grad():\n",
    "    print(b)    #tensor.detach() 方法也可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立神经网络\n",
    "#torch.nn类方法来建立\n",
    "#定义一个拥有可学习参数的神经网络\n",
    "#遍历训练集,处理数据使其流入神经网络\n",
    "#计算损失值,将网络参数的梯度反向传播\n",
    "#根据定义规则更新权重\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#先定义一个神经网络,这里例子是卷积网络\n",
    "class Net(nn.module):\n",
    "    # 继承nn.module父类\n",
    "    #定义初始化函数:\n",
    "    def __init__(self):         #定义层属性\n",
    "        super(Net,self).__init__()  #super方法继承父类设定\n",
    "\n",
    "    def forward(self,x):    #完成向前计算的过程\n",
    "\n",
    "## torch.nn 方法建立的神经网络只支持 mini-batch 的输入,而不支持单一样本\n",
    "##如果输入张量维度与神经网络要求不同,可以通过input.squeeze(n) 来扩充第n维\n",
    "\n",
    "# 损失函数\n",
    "#接受神经网络的输出,计算与target的差距\n",
    "#一般直接调用 nn.lossfunctions 来计算\n",
    "criterion = nn.MSELoss()    #定义损失函数,例子中调用的是MSEloss\n",
    "loss = criterion(output,target)\n",
    "print(loss)\n",
    "print(loss.grad_fn)     #查看loss层的操作\n",
    "print(loss.grad_fn.next_fuinctions[0][0])   #追踪loss前一层的操作\n",
    "print(loss.grad_fn.next_fuinctions[0][0].next_fuinctions[0][0])     #追踪前两层的操作\n",
    "\n",
    " \n",
    "#反向传播,在执行之前要将梯度全部清0,否则会在不同批次的数据间累加\n",
    "net.zero_grad()    #清零梯度\n",
    "print(loss.backward())  #反向传播\n",
    "print(net.layer.bias.grad)  #输出该层bias梯度\n",
    "\n",
    "#更新网络参数\n",
    "#先选择一个算法,这里以SGD实例,它的公式为 weight = weight - learning_rate *gradient\n",
    "#torch.optim 中内带了多种常用的优化算法,一般直接调用\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(),lr = 0.01)   #调用相关算法,输入为网络参数和学习率\n",
    "optimizer.zero_grad()   #清零梯度\n",
    "output = net(input)     #\n",
    "loss = criterion(output, target)\n",
    "loss.backward()     #计算并输出loss\n",
    "optimizer.step()    #由此更新参数\n",
    "\n",
    "\n",
    "#模型结果相关\n",
    "net.parameters() #获取所有 requires_grad = true 的参数\n",
    "net.eval()      #评估模型\n",
    "\n",
    "\n",
    "#数据加载器 dataloader\n",
    "#神经网络的数据量非常庞大,调用该方法来快速加入数据\n",
    "#pytorch 中的数据集类:\n",
    "from torch.utils.data import Dataset\n",
    "data_path = r'...'\n",
    "\n",
    "#调用方法\n",
    "class train_data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.lines = open(data_path).readlines()        #一般把预处理的步骤放在这 \n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.lines[index]        #索引数据\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)          #获取数据数量\n",
    "\n",
    "#实现了上述方法后,可以进行数据迭代,方便后续训练/验证\n",
    "#在torch中使用 dataloader 来实现\n",
    "from torch.utils.data import  DataLoader\n",
    "def get_dataloader(train = True, batch_size = BATCH_SIZE):\n",
    "    train_set = train_data()\n",
    "    data_loader = DataLoader(dataset=train_set, batch_size = n, shuffle = True) #通过随机抽取将数据分成n批来训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立分类器\n",
    "#基本流程和上面的部分一样,常见的训练模板:\n",
    "model = ...             #调用torch模型\n",
    "optimizer = optim.moddel(net.parameters(),lr = x)       #带入损失函数/优化器\n",
    "def training(epoch):        #训练n次\n",
    "    mode = True\n",
    "    model.train(mode = mode)\n",
    "    data_loader = get_dataloader()\n",
    "    for idx, (input,target) in enumerate(data_loader):  #遍历数据集\n",
    "            optimizer.zero_grad()              #归零梯度\n",
    "            outputs = net(input)               #定义输出\n",
    "            loss = criterion(output,labels)    #定义损失\n",
    "            loss.backward()     #反向传播\n",
    "            optimizer.step()    #得到损失，更新梯度\n",
    "            if idx  % n == 0 :           #以每n组为单位打印损失值,方便追踪训练过程\n",
    "                print('[%d, %d] loss: ', %(epoch, idx, loss.item()))\n",
    "\n",
    "#保存模型\n",
    "#先设定保存路径\n",
    "PATH = '/.../.../..'\n",
    "#保存模型状态(输出为字典类)\n",
    "torch.save(net.state_dict().PATH)\n",
    "model.load_state_dict(torch.load(path))     #加载模型\n",
    "\n",
    "\n",
    "#模型测试主要看 损失 和 准确率, 不用计算梯度\n",
    "#在测试集上测试模型,常用流程代码如下:\n",
    "def test():\n",
    "    test_loss = 0       #设定初始损失\n",
    "    correct = 0         #设定初始准确率\n",
    "    model.eval()        #进入验证模式\n",
    "    test_dataloder = get_dataloader(train=False)        #调用验证集\n",
    "    for idx,(input,target) in enumerate(test_dataloder):\n",
    "         with torch.no_grad:                            #不进行梯度计算\n",
    "              output = model(input)                     #设定输出\n",
    "              test_loss = method(output, target)   #调用损失函数   \n",
    "              #计算准确率\n",
    "              pred = output.data.max(1, keepdim = True)[1]      #获取预测值\n",
    "              correct += pred.eq(target.data.view_as(pred)).sum()       #计算准确预测数量\n",
    "    \n",
    "\n",
    "#多分类任务时\n",
    "class_correct = list(0 for i in range(n))\n",
    "class_total = list(0 for i in range(n))\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        data= xxx\n",
    "        label = xxx\n",
    "        output = net(labels)\n",
    "        _, predicted = torch.max(outputs.data,1)    #_, 表示在两个返回值中取后者作为输入\n",
    "        c = (predicted == labels).sqeeze()\n",
    "        for i in range(m):              # m表示批次中数据的数量\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "    for j in range(n):\n",
    "        print('accuracy of %s: %d %%'%(class[i],100*class_correct[i]/class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#在GPU上训练模型\n",
    "#定义设备\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #如果cuda可用就将device设定为gpu, 否则为cpu\n",
    "print(device)\n",
    "\n",
    "#模型训练时\n",
    "net.to(device)  #转移到gpu上训练\n",
    "inputs,labels = data[0].to(device), data[1].to(device)  #在迭代训练的过程中,每一步都将数据和标签张量转移到gpu上\n",
    "\n",
    "predict = predict.cpu().detach().numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch 中自带的embedding生成器\n",
    "'''\n",
    "word_embedding = nn.Embedding(no_of_vocab,embedding_size,padding_idx = int,..)\n",
    "embedding_size = num of label(including paddings)\n",
    "'''\n",
    "import torch\n",
    "#首先需要一个文本\n",
    "text = ['come with me','aye got it','i will come']\n",
    "#接着要对这些文本做标准化处理(停用词/是否去掉标点符号/大小写等)\n",
    "#之后把处理好的文本映射为一个key = word, v = index的字典,即每个单词对应一个唯一的id,而且要去重\n",
    "def word_to_dict(text_list):\n",
    "    #已处理文本转为字典,建立词表\n",
    "    word2dict = {}\n",
    "    cnt = 0\n",
    "    for s in text:\n",
    "        word_list = s.split( )\n",
    "        for word in word_list:\n",
    "            if word not in word2dict:\n",
    "                word2dict[word] = cnt\n",
    "                cnt +=1\n",
    "    return word2dict\n",
    "\n",
    "def word_to_index(word2id_dict):\n",
    "    #通过word2id字典来输出文本的索引\n",
    "    out = []\n",
    "    for s in text:\n",
    "        s_to_id = []\n",
    "        for word in s.split():\n",
    "            if word in word2id_dict:\n",
    "                s_to_id.append(word2id_dict[word])\n",
    "        out.append(s_to_id)\n",
    "    return out\n",
    "\n",
    "#还需要一个函数记录文本长度,如果文本中句子都一样长就不动,不一样就加padding,最后根据\n",
    "#情况加上end/start pading,这里设start id = len(word2dict)+1,end id = len(word2dict)+2,如果文本长度不一样,就按最长文本的长度填充pad,保证所有sample长度相等:\n",
    "#nn.embedding 中有参数 padding_idx可以输入一个数字用来作为长度padding并自动填充\n",
    "def padding_all(text2id,word_dict,end_pad = False,start_pad = False,len_pad = False):\n",
    "    max_len = max([len(s) for s in text2id])\n",
    "    start_pad_id,end_pad_id,len_pad_id= len(word_dict) +1,len(word_dict) +2 ,len(word_dict)+3\n",
    "    if start_pad:\n",
    "        for sample in text2id:\n",
    "            sample.insert(0,start_pad_id)\n",
    "\n",
    "\n",
    "    if end_pad:\n",
    "        for sample in text2id:\n",
    "            sample.append(end_pad_id)\n",
    "\n",
    "    if len_pad:\n",
    "        for sample in text2id:\n",
    "            if len(sample)<max_len:\n",
    "                sample.append(len_pad_id)\n",
    "                \n",
    "\n",
    "    return text2id\n",
    "word_dict = word_to_dict(text)\n",
    "text2id = word_to_index(word_dict)\n",
    "text2id = padding_all(text2id,word_dict,start_pad=True,len_pad=True)\n",
    "word_dict = word_to_dict(text)\n",
    "text2id = word_to_index(word_dict)\n",
    "text2id = padding_all(text2id,word_dict,start_pad=True,end_pad=True,len_pad=True)\n",
    "#一定要记住加了几个pad就要在词表里加进去\n",
    "#否则后面会因为index超过范围报错(传入的batch中的值会超过词表长度)\n",
    "word_dict['start_pad'] = 9\n",
    "word_dict['end_pad'] = 10\n",
    "word_dict['len_pad'] = 11\n",
    "\n",
    "#全部处理好之后,要把他们封装进batch中,batch的维度根据不同模型要求有变化\n",
    "#我们的原始batch,即处理好的text2id有3句话,每句话有4个词(要算上padding)\n",
    "#具体batch的维度要和使用的模型相同,对于RNN,他要求我们每次同时输入 每个句子的一个词\n",
    "#原始text2id的维度经过padding是4*3,RNN要求则是 3(每次输入这3个句子的一个词) *4(重复4次完成全部输入)\n",
    "import itertools\n",
    " #循环提取每个句子的每个词组成和模型对齐的维度\n",
    "batch_from_text2id =list(itertools.zip_longest(text2id[0],text2id[1],text2id[2]))\n",
    "#处理好后要把batch转换为张量\n",
    "batch_from_text2id = torch.LongTensor(batch_from_text2id)\n",
    "#建立词向量层,传入词表长度和embedding size,embedding size不能大于词表中的最大index:\n",
    "embed = nn.Embedding(len(word_dict),embedding_dim= 9)\n",
    "#把处理好的batch传入embedding层:\n",
    "embed_batch = embed(batch_from_text2id)  #输出为一个[seq_len x batch_len x input_size]的张量\n",
    "embed_batch.shape #最后一个值就是输入的input_size,即输入大小(不是输入形状)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2\n",
      "6 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#杰卡德系数\n",
    "def jaccard_sim(a,b):\n",
    "    unions = len(set(a).union(set(b)))  #取并集\n",
    "    intersections = len(set(a).intersection(set(b))) #取交集\n",
    "    print(unions,intersections)\n",
    "    return intersections/unions\n",
    "#杰卡德距离\n",
    "def jaccard_dis(a,b):\n",
    "    return 1-jaccard_sim(a,b)\n",
    "a = [1,2,4,5]\n",
    "b = [1,9,4,6]\n",
    "jaccard_sim(a,b)\n",
    "jaccard_dis(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
